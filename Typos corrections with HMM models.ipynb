{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuO2bOCE8FnR"
   },
   "source": [
    "# Correction des typos\n",
    "\n",
    "Le but de ce projet est de corriger les fautes de frappes dans un texte sans recours à un dictionnaire, en utilisant un modèle de Markov caché (HMM).\n",
    "\n",
    "# I - Chargement des données\n",
    "\n",
    "Données issues du *Manifeste de l'Unabomber* et artificiellement bruitées en mofifiant aléatoirement 10% ou 20% des lettres du corpus (erreurs de substitutions seulement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from HMM import *\n",
    "from toolbox import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 states :\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "26 observations :\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "Sample example (observation, état) :\n",
      "[('a', 'a'), ('c', 'c'), ('v', 'c'), ('o', 'o'), ('u', 'u'), ('n', 'n'), ('t', 't')]\n"
     ]
    }
   ],
   "source": [
    "# Import and separate datasets\n",
    "ERROR_RATE = 10  # 10% or 20%\n",
    "train_set, test_set = load_db(error_rate=ERROR_RATE)\n",
    "X_train = [[token[0] for token in word] for word in train_set]\n",
    "y_train = [[token[1] for token in word] for word in train_set]\n",
    "X_test = [[token[0] for token in word] for word in test_set]\n",
    "y_test = [[token[1] for token in word] for word in test_set]\n",
    "\n",
    "# Get states and observations sets\n",
    "states, observations = get_observations_states(X_train, y_train)\n",
    "print(\"{} states :\\n{}\".format(len(states), states))\n",
    "print(\"{} observations :\\n{}\".format(len(observations), observations))\n",
    "\n",
    "# Example from dataset\n",
    "print(\"\\nSample example (observation, état) :\\n{}\".format(train_set[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - HMM d'ordre 1\n",
    "\n",
    "Essai de correction des typos par un HMM d'ordre 1 utilisant l'algorithme de Viterbi.\n",
    "On prend en compte un seul instant précédent pour le calcul de la transition. On considère donc les probabilités :\n",
    "- $P(X_t| Y_t)$ : probabilité d'observer X à l'état Y à l'instant t\n",
    "- $P(Y_t| Y_{t-1})$ : probabilité de l'état Y à l'instant t sachant  l'état précédent\n",
    "\n",
    "Afin d'éviter les probabilités nulles, on initialise toutes les matrices (émission, transition et état initial) par une probabilité uniforme (respectivement $1/n_{observations}$, $1/n_{states}$, $1/n_{states}$), puis on affine ces probabilités grâce aux comptes des unigrammes et bigrammes selon le maximum de vraisemblance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st order HMM created with: \n",
      " * 26 states\n",
      " * 26 observations\n",
      "Training initial states probabilities... Done.\n",
      "Training transitions probabilities given states... Done.\n",
      "Training observations probabilities given states... Done.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train HMM\n",
    "hmm1 = HMM(states, observations)\n",
    "hmm1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation sequence      : inferikrigy\n",
      "Real states sequence      : inferiority\n",
      "Predicted states sequence : inderiorigy\n"
     ]
    }
   ],
   "source": [
    "# Try HMM prediction on one sample\n",
    "SAMPLE = 11\n",
    "observation_sequence = X_test[SAMPLE]\n",
    "states_sequence = y_test[SAMPLE]\n",
    "\n",
    "predicted_states_sequence = hmm1.predict([observation_sequence])\n",
    "\n",
    "print(\"Observation sequence      : {}\".format(\"\".join(observation_sequence)))\n",
    "print(\"Real states sequence      : {}\".format(\"\".join(states_sequence)))\n",
    "print(\"Predicted states sequence : {}\".format(\"\".join(predicted_states_sequence[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM1 score on test set\n",
      " * accuracy on full words : 75.15%\n",
      " * accuracy on letters    : 93.20%\n",
      "   > typos corrected      : 310 (4.23%)\n",
      "   > typos not corrected  : 435 (5.94%)\n",
      "   > typos added          : 63 (0.86%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 62.89%\n",
      " * accuracy on letters    : 89.82%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 745 (10.18%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = hmm1.predict(X_test)\n",
    "display_correction_stats(X_test, y_test, y_test_pred, name=\"HMM1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YO0E0GvD8FnS"
   },
   "source": [
    "\n",
    "# III - HMM d'ordre 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM d'ordre 2\n",
    "\n",
    "On prend en compte les deux instants précédents pour le calcul de la transition. On considère donc maitenant les probabilités :\n",
    "- $P(X_t| Y_t)$ : probabilité d'observer X à l'état Y à l'instant t (identique à l'ordre 1)\n",
    "- $P(Y_t| Y_{t-1}, Y_{t-2})$ : probabilité de l'état Y à l'instant t sachant les deux états précédents.\n",
    "\n",
    "La dimension de la matrice de transition étant plus importante, les risques que certains trigrammes n'aient pas été observés dans le corpus d'apprentissage sont encore plus grands. On peut donc tester plusieurs sortes de *smoothing* pour extrapoler ces données manquantes, et voir l'importance de ces traitements.\n",
    "\n",
    "## Aucun smoothing\n",
    "\n",
    "Les matrices sont toutes initialisées à 0. Si certains comptes sont manquants, beaucoup des éléments de la matrice de transition seront nuls (ce qui provoque de belles erreurs à cause des logarithmes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd order HMM created with: \n",
      " * 26 states\n",
      " * 26 observations\n",
      "Training initial states probabilities... Done.\n",
      "Training transitions probabilities given states..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nicolas/Data/Documents/Scolarité/ENSTA/3A/AIC_UPSay/cours/TC4/spelling_corrector/HMM.py:300: RuntimeWarning: divide by zero encountered in log\n",
      "  self.initial_state_logproba = np.log(self.initial_state_logproba)\n",
      "/media/nicolas/Data/Documents/Scolarité/ENSTA/3A/AIC_UPSay/cours/TC4/spelling_corrector/HMM.py:648: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.transition2_logproba /= np.atleast_3d(np.sum(self.transition2_logproba, axis=2))\n",
      "/media/nicolas/Data/Documents/Scolarité/ENSTA/3A/AIC_UPSay/cours/TC4/spelling_corrector/HMM.py:651: RuntimeWarning: divide by zero encountered in log\n",
      "  self.transition1_logproba = np.log(self.transition1_logproba)\n",
      "/media/nicolas/Data/Documents/Scolarité/ENSTA/3A/AIC_UPSay/cours/TC4/spelling_corrector/HMM.py:652: RuntimeWarning: divide by zero encountered in log\n",
      "  self.transition2_logproba = np.log(self.transition2_logproba)\n",
      "/media/nicolas/Data/Documents/Scolarité/ENSTA/3A/AIC_UPSay/cours/TC4/spelling_corrector/HMM.py:330: RuntimeWarning: divide by zero encountered in log\n",
      "  self.observation_logproba = np.log(self.observation_logproba).astype(self.fp_precision)\n",
      "/home/nicolas/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n",
      "Training observations probabilities given states... Done.\n",
      "\n",
      "Observation sequence      : inferikrigy\n",
      "Real states sequence      : inferiority\n",
      "Predicted states sequence : aaaaaaaaaaa\n",
      "\n",
      "HMM2_no_smoothing score on test set\n",
      " * accuracy on full words : 20.65%\n",
      " * accuracy on letters    : 15.16%\n",
      "   > typos corrected      : 71 (0.97%)\n",
      "   > typos not corrected  : 674 (9.21%)\n",
      "   > typos added          : 5536 (75.63%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 62.89%\n",
      " * accuracy on letters    : 89.82%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 745 (10.18%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train HMM\n",
    "hmm2 = HMM2(states, observations)\n",
    "hmm2.fit(X_train, y_train, smoothing=None)\n",
    "\n",
    "# Try HMM prediction on one sample\n",
    "predicted_states_sequence = hmm2.predict([observation_sequence])\n",
    "print(\"\\nObservation sequence      : {}\".format(\"\".join(observation_sequence)))\n",
    "print(\"Real states sequence      : {}\".format(\"\".join(states_sequence)))\n",
    "print(\"Predicted states sequence : {}\\n\".format(\"\".join(predicted_states_sequence[0])))\n",
    "\n",
    "# Run prediction on all test set\n",
    "y_test_pred = hmm2.predict(X_test)\n",
    "display_correction_stats(X_test, y_test, y_test_pred, name=\"HMM2_no_smoothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation non nulle des matrices\n",
    "\n",
    "Afin d'éviter les probabilités nulles, on initialise toutes les matrices (émission, transition et état initial) par une probabilité uniforme (respectivement $1/n_{observations}$, $1/n_{states}$, $1/n_{states}$), puis on affine ces probabilités grâce aux comptes d'unigrammes, bigrammes et trigrammes selon le maximum de vraisemblance. On normalise ensuite les distributions de probabilités obtenues.\n",
    "\n",
    "Il s'agit du comportement de base de nos HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd order HMM created with: \n",
      " * 26 states\n",
      " * 26 observations\n",
      "Training initial states probabilities... Done.\n",
      "Training transitions probabilities given states... Done.\n",
      "Training observations probabilities given states... Done.\n",
      "\n",
      "Observation sequence      : inferikrigy\n",
      "Real states sequence      : inferiority\n",
      "Predicted states sequence : inferiority\n",
      "\n",
      "HMM2_epsilon_smoothing score on test set\n",
      " * accuracy on full words : 83.41%\n",
      " * accuracy on letters    : 95.57%\n",
      "   > typos corrected      : 513 (7.01%)\n",
      "   > typos not corrected  : 232 (3.17%)\n",
      "   > typos added          : 92 (1.26%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 62.89%\n",
      " * accuracy on letters    : 89.82%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 745 (10.18%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train HMM\n",
    "hmm2 = HMM2(states, observations)\n",
    "hmm2.fit(X_train, y_train, smoothing='epsilon')\n",
    "\n",
    "# Try HMM prediction on one sample\n",
    "predicted_states_sequence = hmm2.predict([observation_sequence])\n",
    "print(\"\\nObservation sequence      : {}\".format(\"\".join(observation_sequence)))\n",
    "print(\"Real states sequence      : {}\".format(\"\".join(states_sequence)))\n",
    "print(\"Predicted states sequence : {}\\n\".format(\"\".join(predicted_states_sequence[0])))\n",
    "\n",
    "# Run prediction on all test set\n",
    "y_test_pred = hmm2.predict(X_test)\n",
    "display_correction_stats(X_test, y_test, y_test_pred, name=\"HMM2_epsilon_smoothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle d'extrapolation plus complexe\n",
    "\n",
    "Comme de nombreux trigrammes peuvent ne pas avoir été rencontrés dans le corpus d'apprentissage, on peut introduire différentes techniques de lissage ou d'extrapolation des données. On teste ici la technique introduite par [cet article](http://www.aclweb.org/anthology/P99-1023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd order HMM created with: \n",
      " * 26 states\n",
      " * 26 observations\n",
      "Training initial states probabilities... Done.\n",
      "Training transitions probabilities given states... Done.\n",
      "Training observations probabilities given states... Done.\n",
      "\n",
      "Observation sequence      : inferikrigy\n",
      "Real states sequence      : inferiority\n",
      "Predicted states sequence : inferiorigy\n",
      "\n",
      "HMM2_weighted_smoothing score on test set\n",
      " * accuracy on full words : 79.61%\n",
      " * accuracy on letters    : 94.66%\n",
      "   > typos corrected      : 406 (5.55%)\n",
      "   > typos not corrected  : 339 (4.63%)\n",
      "   > typos added          : 52 (0.71%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 62.89%\n",
      " * accuracy on letters    : 89.82%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 745 (10.18%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train HMM\n",
    "hmm2 = HMM2(states, observations)\n",
    "hmm2.fit(X_train, y_train, smoothing='weighted')\n",
    "\n",
    "# Try HMM prediction on one sample\n",
    "predicted_states_sequence = hmm2.predict([observation_sequence])\n",
    "print(\"\\nObservation sequence      : {}\".format(\"\".join(observation_sequence)))\n",
    "print(\"Real states sequence      : {}\".format(\"\".join(states_sequence)))\n",
    "print(\"Predicted states sequence : {}\\n\".format(\"\".join(predicted_states_sequence[0])))\n",
    "\n",
    "# Run prediction on all test set\n",
    "y_test_pred = hmm2.predict(X_test)\n",
    "display_correction_stats(X_test, y_test, y_test_pred, name=\"HMM2_weighted_smoothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Finalement moins efficace, mais évite de rajouter beaucoup d'erreurs (comportement plus agréable pour l'utilisateur)\n",
    "\n",
    "# IV - Insertion de caractères\n",
    "\n",
    "## Adaptation du modèle\n",
    "\n",
    "Pour simuler l'insertion de caractères, on ajoute avec un certaine probabilité un caractère (dont la touche du clavier est proche de la précédente). Comme cette observation supplémentaire ne correspond pas un état (pas de véritable lettre associée à cette insertion), on modélise celui-ci par un nouvel état '\\_'. On se retrouve face à un problème de 27 états (26 lettres + '\\_') et 26 observations.\n",
    "\n",
    "Exemple:\n",
    "- **séquence d'états        :** ['a', 'c', 'c', 'o', '\\_', 'u', 'n', 't']\n",
    "- **séquence d'observations :** ['a', 'c', 'c', 'o', 'p', 'u', 'n', 't']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States sequence sample       : ['a', 'c', 'c', 'o', 'u', '_', 'n', 't']\n",
      "Observations sequence sample : ['a', 'c', 'v', 'o', 'u', 'u', 'n', 't']\n",
      "\n",
      "27 states :\n",
      "['_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "26 observations :\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Add noisy insertions (linked to no state) to the noisy substituted or clean database\n",
    "# Empty state will be denoted '_'\n",
    "np.random.seed(20)\n",
    "INSERTION_PROB = 0.10\n",
    "(X_si_train, y_si_train), (X_si_test, y_si_test) = noisy_insertion(X_train, y_train, X_test, y_test, thresh_proba=INSERTION_PROB)\n",
    "(X_i_train, y_i_train), (X_i_test, y_i_test) = noisy_insertion(y_train, y_train, y_test, y_test, thresh_proba=INSERTION_PROB)\n",
    "\n",
    "SAMPLE = 3\n",
    "print(\"States sequence sample       : {}\".format(y_si_train[SAMPLE]))\n",
    "print(\"Observations sequence sample : {}\\n\".format(X_si_train[SAMPLE]))\n",
    "\n",
    "states_si, observations_si = get_observations_states(X_si_train, y_si_train)\n",
    "print(\"{} states :\\n{}\".format(len(states_si), states_si))\n",
    "print(\"{} observations :\\n{}\".format(len(observations_si), observations_si))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests de correction des erreurs de substitution + insertion\n",
    "\n",
    "10% d'erreurs de substitutions + 10% d'erreurs d'insertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM1 score on test set\n",
      " * accuracy on full words : 49.83%\n",
      " * accuracy on letters    : 83.64%\n",
      "   > typos corrected      : 391 (4.85%)\n",
      "   > typos not corrected  : 1091 (13.54%)\n",
      "   > typos added          : 227 (2.82%)\n",
      "\n",
      "HMM2 score on test set\n",
      " * accuracy on full words : 60.56%\n",
      " * accuracy on letters    : 85.90%\n",
      "   > typos corrected      : 677 (8.40%)\n",
      "   > typos not corrected  : 805 (9.99%)\n",
      "   > typos added          : 331 (4.11%)\n",
      "\n",
      "HMM2_smooth score on test set\n",
      " * accuracy on full words : 55.16%\n",
      " * accuracy on letters    : 85.61%\n",
      "   > typos corrected      : 499 (6.19%)\n",
      "   > typos not corrected  : 983 (12.20%)\n",
      "   > typos added          : 176 (2.18%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 41.77%\n",
      " * accuracy on letters    : 81.61%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 1482 (18.39%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# HMM first order\n",
    "hmm = HMM(states_si, observations_si, verbose=False)\n",
    "hmm.fit(X_si_train, y_si_train)\n",
    "y_si_test_pred = hmm.predict(X_si_test)\n",
    "display_correction_stats(X_si_test, y_si_test, y_si_test_pred, name=\"HMM1\", dummy=False)\n",
    "\n",
    "# HMM second order\n",
    "hmm2 = HMM2(states_si, observations_si, verbose=False)\n",
    "hmm2.fit(X_si_train, y_si_train)\n",
    "y_si_test_pred = hmm2.predict(X_si_test)\n",
    "display_correction_stats(X_si_test, y_si_test, y_si_test_pred, name=\"\\nHMM2\", dummy=False)\n",
    "\n",
    "# HMM second order with smoothing\n",
    "hmm2 = HMM2(states_si, observations_si, verbose=False)\n",
    "hmm2.fit(X_si_train, y_si_train, smoothing='weighted')\n",
    "y_si_test_pred = hmm2.predict(X_si_test)\n",
    "display_correction_stats(X_si_test, y_si_test, y_si_test_pred, name=\"\\nHMM2_smooth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de correction des erreurs d'insertions seulement\n",
    "\n",
    "10% d'erreurs d'insertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM1 score on test set\n",
      " * accuracy on full words : 66.02%\n",
      " * accuracy on letters    : 90.77%\n",
      "   > typos corrected      : 189 (2.34%)\n",
      "   > typos not corrected  : 565 (7.00%)\n",
      "   > typos added          : 180 (2.23%)\n",
      "\n",
      "HMM2 score on test set\n",
      " * accuracy on full words : 73.48%\n",
      " * accuracy on letters    : 91.37%\n",
      "   > typos corrected      : 342 (4.24%)\n",
      "   > typos not corrected  : 412 (5.10%)\n",
      "   > typos added          : 285 (3.53%)\n",
      "\n",
      "HMM2_smooth score on test set\n",
      " * accuracy on full words : 66.82%\n",
      " * accuracy on letters    : 91.16%\n",
      "   > typos corrected      : 158 (1.96%)\n",
      "   > typos not corrected  : 596 (7.38%)\n",
      "   > typos added          : 118 (1.46%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 60.89%\n",
      " * accuracy on letters    : 90.66%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 754 (9.34%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# HMM first order\n",
    "hmm = HMM(states_si, observations_si, verbose=False)\n",
    "hmm.fit(X_i_train, y_i_train)\n",
    "y_i_test_pred = hmm.predict(X_i_test)\n",
    "display_correction_stats(X_i_test, y_i_test, y_i_test_pred, name=\"HMM1\", dummy=False)\n",
    "\n",
    "# HMM second order\n",
    "hmm2 = HMM2(states_si, observations_si, verbose=False)\n",
    "hmm2.fit(X_i_train, y_i_train)\n",
    "y_i_test_pred = hmm2.predict(X_i_test)\n",
    "display_correction_stats(X_i_test, y_i_test, y_i_test_pred, name=\"\\nHMM2\", dummy=False)\n",
    "\n",
    "# HMM second order with smoothing\n",
    "hmm2 = HMM2(states_si, observations_si, verbose=False)\n",
    "hmm2.fit(X_i_train, y_i_train, smoothing='weighted')\n",
    "y_i_test_pred = hmm2.predict(X_i_test)\n",
    "display_correction_stats(X_i_test, y_i_test, y_i_test_pred, name=\"\\nHMM2_smooth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion :**\n",
    "\n",
    "Résultats assez décevants... En réalité, on remarque que les performance du modèle sont à peine meilleures que le *Dummy* pour la correction des insertions. Ceci est probablement dû aux peu de données d'apprentissage disponibles pour ce type d'erreurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V - Omission de caractères\n",
    "\n",
    "Pour l'omission (délétion) de caractères, on est dans le cas contraire, où une observation est manquante. On ne peut simplement rajouter une observation '\\_', puisque cela reviendrait à indiquer directement au modèle qu'il y a eu une omission.  On serait alors dans un cas plus simple, mais où le modèle n'aurait pas à déterminer tout seul la résence d'une omission de caractère. Pour ceci, on modifie plutôt le corpus en supprimant certaines observations, et en autorisant le remplacement d'un observation par un état \"double\", composé de deux lettres. On se retrouve face à un problème beaucoup plus complexe, de 702 états (26 états simples + 676 états doubles) et 26 observations.\n",
    "\n",
    "Exemple:\n",
    "- **séquence d'états        :** ['a', 'c', 'c', 'o', 'un', 't']\n",
    "- **séquence d'observations :** ['a', 'c', 'c', 'o', 'u', 't']\n",
    "\n",
    "Remarque: de nombreuses lignes de la matrice de transition seront vides, puisque chacune de ces fautes est rarement observée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_states(states):\n",
    "    \"\"\" Take a state list as input and return a cross state list\n",
    "    (each new state being a concatenation of two states from states, or a unique state)\n",
    "    Ex: If states=['a', 'b'], cross_states is ['a', 'b', 'aa', 'ab', 'ba', 'bb']\n",
    "\n",
    "    :param states, list of string\n",
    "    :return cross_states: states, list of string\n",
    "    \"\"\"\n",
    "    \n",
    "    cross_states = []\n",
    "    \n",
    "    for s1 in states:\n",
    "        for s2 in states:\n",
    "            cross_states.append(s1 + s2)\n",
    "            \n",
    "    return states + cross_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States sequence sample       : ['a', 'c', 'co', 'u', 'n', 't']\n",
      "Observations sequence sample : ['a', 'c', 'o', 'u', 'n', 't']\n",
      "\n",
      "702 states :\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az', 'ba', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bh', 'bi', 'bj', 'bk', 'bl', 'bm', 'bn', 'bo', 'bp', 'bq', 'br', 'bs', 'bt', 'bu', 'bv', 'bw', 'bx', 'by', 'bz', 'ca', 'cb', 'cc', 'cd', 'ce', 'cf', 'cg', 'ch', 'ci', 'cj', 'ck', 'cl', 'cm', 'cn', 'co', 'cp', 'cq', 'cr', 'cs', 'ct', 'cu', 'cv', 'cw', 'cx', 'cy', 'cz', 'da', 'db', 'dc', 'dd', 'de', 'df', 'dg', 'dh', 'di', 'dj', 'dk', 'dl', 'dm', 'dn', 'do', 'dp', 'dq', 'dr', 'ds', 'dt', 'du', 'dv', 'dw', 'dx', 'dy', 'dz', 'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'eh', 'ei', 'ej', 'ek', 'el', 'em', 'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ew', 'ex', 'ey', 'ez', 'fa', 'fb', 'fc', 'fd', 'fe', 'ff', 'fg', 'fh', 'fi', 'fj', 'fk', 'fl', 'fm', 'fn', 'fo', 'fp', 'fq', 'fr', 'fs', 'ft', 'fu', 'fv', 'fw', 'fx', 'fy', 'fz', 'ga', 'gb', 'gc', 'gd', 'ge', 'gf', 'gg', 'gh', 'gi', 'gj', 'gk', 'gl', 'gm', 'gn', 'go', 'gp', 'gq', 'gr', 'gs', 'gt', 'gu', 'gv', 'gw', 'gx', 'gy', 'gz', 'ha', 'hb', 'hc', 'hd', 'he', 'hf', 'hg', 'hh', 'hi', 'hj', 'hk', 'hl', 'hm', 'hn', 'ho', 'hp', 'hq', 'hr', 'hs', 'ht', 'hu', 'hv', 'hw', 'hx', 'hy', 'hz', 'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'ih', 'ii', 'ij', 'ik', 'il', 'im', 'in', 'io', 'ip', 'iq', 'ir', 'is', 'it', 'iu', 'iv', 'iw', 'ix', 'iy', 'iz', 'ja', 'jb', 'jc', 'jd', 'je', 'jf', 'jg', 'jh', 'ji', 'jj', 'jk', 'jl', 'jm', 'jn', 'jo', 'jp', 'jq', 'jr', 'js', 'jt', 'ju', 'jv', 'jw', 'jx', 'jy', 'jz', 'ka', 'kb', 'kc', 'kd', 'ke', 'kf', 'kg', 'kh', 'ki', 'kj', 'kk', 'kl', 'km', 'kn', 'ko', 'kp', 'kq', 'kr', 'ks', 'kt', 'ku', 'kv', 'kw', 'kx', 'ky', 'kz', 'la', 'lb', 'lc', 'ld', 'le', 'lf', 'lg', 'lh', 'li', 'lj', 'lk', 'll', 'lm', 'ln', 'lo', 'lp', 'lq', 'lr', 'ls', 'lt', 'lu', 'lv', 'lw', 'lx', 'ly', 'lz', 'ma', 'mb', 'mc', 'md', 'me', 'mf', 'mg', 'mh', 'mi', 'mj', 'mk', 'ml', 'mm', 'mn', 'mo', 'mp', 'mq', 'mr', 'ms', 'mt', 'mu', 'mv', 'mw', 'mx', 'my', 'mz', 'na', 'nb', 'nc', 'nd', 'ne', 'nf', 'ng', 'nh', 'ni', 'nj', 'nk', 'nl', 'nm', 'nn', 'no', 'np', 'nq', 'nr', 'ns', 'nt', 'nu', 'nv', 'nw', 'nx', 'ny', 'nz', 'oa', 'ob', 'oc', 'od', 'oe', 'of', 'og', 'oh', 'oi', 'oj', 'ok', 'ol', 'om', 'on', 'oo', 'op', 'oq', 'or', 'os', 'ot', 'ou', 'ov', 'ow', 'ox', 'oy', 'oz', 'pa', 'pb', 'pc', 'pd', 'pe', 'pf', 'pg', 'ph', 'pi', 'pj', 'pk', 'pl', 'pm', 'pn', 'po', 'pp', 'pq', 'pr', 'ps', 'pt', 'pu', 'pv', 'pw', 'px', 'py', 'pz', 'qa', 'qb', 'qc', 'qd', 'qe', 'qf', 'qg', 'qh', 'qi', 'qj', 'qk', 'ql', 'qm', 'qn', 'qo', 'qp', 'qq', 'qr', 'qs', 'qt', 'qu', 'qv', 'qw', 'qx', 'qy', 'qz', 'ra', 'rb', 'rc', 'rd', 're', 'rf', 'rg', 'rh', 'ri', 'rj', 'rk', 'rl', 'rm', 'rn', 'ro', 'rp', 'rq', 'rr', 'rs', 'rt', 'ru', 'rv', 'rw', 'rx', 'ry', 'rz', 'sa', 'sb', 'sc', 'sd', 'se', 'sf', 'sg', 'sh', 'si', 'sj', 'sk', 'sl', 'sm', 'sn', 'so', 'sp', 'sq', 'sr', 'ss', 'st', 'su', 'sv', 'sw', 'sx', 'sy', 'sz', 'ta', 'tb', 'tc', 'td', 'te', 'tf', 'tg', 'th', 'ti', 'tj', 'tk', 'tl', 'tm', 'tn', 'to', 'tp', 'tq', 'tr', 'ts', 'tt', 'tu', 'tv', 'tw', 'tx', 'ty', 'tz', 'ua', 'ub', 'uc', 'ud', 'ue', 'uf', 'ug', 'uh', 'ui', 'uj', 'uk', 'ul', 'um', 'un', 'uo', 'up', 'uq', 'ur', 'us', 'ut', 'uu', 'uv', 'uw', 'ux', 'uy', 'uz', 'va', 'vb', 'vc', 'vd', 've', 'vf', 'vg', 'vh', 'vi', 'vj', 'vk', 'vl', 'vm', 'vn', 'vo', 'vp', 'vq', 'vr', 'vs', 'vt', 'vu', 'vv', 'vw', 'vx', 'vy', 'vz', 'wa', 'wb', 'wc', 'wd', 'we', 'wf', 'wg', 'wh', 'wi', 'wj', 'wk', 'wl', 'wm', 'wn', 'wo', 'wp', 'wq', 'wr', 'ws', 'wt', 'wu', 'wv', 'ww', 'wx', 'wy', 'wz', 'xa', 'xb', 'xc', 'xd', 'xe', 'xf', 'xg', 'xh', 'xi', 'xj', 'xk', 'xl', 'xm', 'xn', 'xo', 'xp', 'xq', 'xr', 'xs', 'xt', 'xu', 'xv', 'xw', 'xx', 'xy', 'xz', 'ya', 'yb', 'yc', 'yd', 'ye', 'yf', 'yg', 'yh', 'yi', 'yj', 'yk', 'yl', 'ym', 'yn', 'yo', 'yp', 'yq', 'yr', 'ys', 'yt', 'yu', 'yv', 'yw', 'yx', 'yy', 'yz', 'za', 'zb', 'zc', 'zd', 'ze', 'zf', 'zg', 'zh', 'zi', 'zj', 'zk', 'zl', 'zm', 'zn', 'zo', 'zp', 'zq', 'zr', 'zs', 'zt', 'zu', 'zv', 'zw', 'zx', 'zy', 'zz']\n",
      "26 observations :\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Remove some observations in the database\n",
    "# To do that, we add the state without observation to the previous state\n",
    "np.random.seed(10)\n",
    "DELETION_PROB = 0.10\n",
    "(X_so_train, y_so_train), (X_so_test, y_so_test) = noisy_omission(X_train, y_train, X_test, y_test, thresh_proba=DELETION_PROB)\n",
    "(X_o_train, y_o_train), (X_o_test, y_o_test) = noisy_omission(y_train, y_train, y_test, y_test, thresh_proba=DELETION_PROB)\n",
    "\n",
    "states_so = cross_states(states)\n",
    "observations_so = observations\n",
    "\n",
    "SAMPLE = 3\n",
    "print(\"States sequence sample       : {}\".format(y_so_train[SAMPLE]))\n",
    "print(\"Observations sequence sample : {}\\n\".format(X_so_train[SAMPLE]))\n",
    "\n",
    "print(\"{} states :\\n{}\".format(len(states_so), states_so))\n",
    "print(\"{} observations :\\n{}\".format(len(observations_so), observations_so))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests de correction des erreurs de substitution + omission\n",
    "\n",
    "10% d'erreurs de substitutions + 10% d'erreurs d'omissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM score on test set\n",
      " * accuracy on full words : 55.16%\n",
      " * accuracy on letters    : 84.95%\n",
      "   > typos corrected      : 292 (4.33%)\n",
      "   > typos not corrected  : 916 (13.57%)\n",
      "   > typos added          : 100 (1.48%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 46.30%\n",
      " * accuracy on letters    : 82.10%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 1208 (17.90%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# HMM first order (omissions + substitutions)\n",
    "hmm = HMM(states_so, observations_so, verbose=False)\n",
    "hmm.fit(X_so_train, y_so_train)\n",
    "y_so_test_pred = hmm.predict(X_so_test)\n",
    "display_correction_stats(X_so_test, y_so_test, y_so_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de correction des erreurs d'omissions seulement\n",
    "\n",
    "10% d'erreurs d'omissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM score on test set\n",
      " * accuracy on full words : 71.89%\n",
      " * accuracy on letters    : 92.58%\n",
      "   > typos corrected      : 88 (1.30%)\n",
      "   > typos not corrected  : 479 (7.09%)\n",
      "   > typos added          : 22 (0.33%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 68.75%\n",
      " * accuracy on letters    : 91.60%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 567 (8.40%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# HMM first order (omissions only)\n",
    "hmm = HMM(states_so, observations_so, verbose=False)\n",
    "hmm.fit(X_o_train, y_o_train)\n",
    "y_o_test_pred = hmm.predict(X_o_test)\n",
    "display_correction_stats(X_o_test, y_o_test, y_o_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**/!\\ Attention /!\\**\n",
    "\n",
    "Attention, dans le cas de l'ordre 2, les matrices considérées peuvent prendre des tailles très importantes. Ici, la matrice de transition est de taille 702 x 702 x 702, ce qui dans le cas de flottants sur 64 bits (type de base d'un array numpy), revient à une matrice de plus de 2.5 Go. Afin d'éviter des erreurs de mémoire et limiter les calculs, la précision des nombres flottants est automatiquement modifiée en fonction de la dimension du problème, aboutissant ici à des matrices de \"seulement\" 600 Mo. Pour compenser cette perte de pécision, l'usage des log-probabilités est fortement recommandé (et effectué ici), permettant de mieux exploiter la plage des valeurs possibles, et ainsi limiter les erreurs de représentations.\n",
    "\n",
    "Malgré l'utilisation du *broadcasting* numpy, l'algorithme de Viterbi reste très lent, avec plus d'une dizaine de secondes par séquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd order HMM created with: \n",
      " * 702 states\n",
      " * 26 observations\n",
      "Training initial states probabilities... Done.\n",
      "Training transitions probabilities given states... Done.\n",
      "Training observations probabilities given states... Done.\n",
      "\n",
      "Observation sequence      : hmself\n",
      "Real states sequence      : himself\n",
      "Predicted states sequence : himself\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HMM second order (omissions only)\n",
    "hmm2 = HMM2(states_so, observations_so)\n",
    "hmm2.fit(X_o_train, y_o_train)\n",
    "\n",
    "# Try HMM prediction on one sample\n",
    "SAMPLE = 20\n",
    "print(\"\\nObservation sequence      : {}\".format(\"\".join(X_o_test[SAMPLE])))\n",
    "print(\"Real states sequence      : {}\".format(\"\".join(y_o_test[SAMPLE])))\n",
    "print(\"Predicted states sequence : {}\\n\".format(\"\".join(hmm2.predict([X_o_test[SAMPLE]])[0])))\n",
    "\n",
    "# # Run all predictions (BE CAREFUL, IT MAY BE VEEEERY LONG)\n",
    "# y_so_test_pred = hmm2.predict(X_so_test)\n",
    "# display_correction_stats(X_so_test, y_so_test, y_so_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI - Apprentissage non-supervisé\n",
    "\n",
    "Apprentissage non supervisé par algorithme de Baum-Welch (~ Expectation Maximization).\n",
    "Réalisé pour le HMM d'ordre 1, mais pas pour l'ordre 2 (serait de toute façon beaucoup trop long à calculer dans notre cas...).\n",
    "\n",
    "## Sans initialisation a priori\n",
    "\n",
    "Les matrices sont initialisés aléatoirement autour des valeurs $1/n_{observations}$ et $1/n_{states}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st order HMM created with: \n",
      " * 26 states\n",
      " * 26 observations\n",
      "Unsupervised training by EM algorithm...\n",
      "EM LOOP: n_iter=1, delta=0.0014\n",
      "EM LOOP: n_iter=2, delta=0.0013\n",
      "EM LOOP: n_iter=3, delta=0.0009\n",
      "\n",
      "HMM1 score on test set\n",
      " * accuracy on full words : 0.00%\n",
      " * accuracy on letters    : 0.53%\n",
      "   > typos corrected      : 17 (0.23%)\n",
      "   > typos not corrected  : 728 (9.95%)\n",
      "   > typos added          : 6553 (89.52%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 62.89%\n",
      " * accuracy on letters    : 89.82%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 745 (10.18%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize 1st order HMM\n",
    "hmm1 = HMM(states, observations)\n",
    "\n",
    "# Train HMM using EM algorithm\n",
    "hmm1.fit(X_train, max_iter=5, tol=0.001)\n",
    "\n",
    "# Run predictions\n",
    "y_test_pred = hmm1.predict(X_test)\n",
    "display_correction_stats(X_test, y_test, y_test_pred, name=\"\\nHMM1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vérifier nos résultats, nous avons testé ceux obtenus avec la librairie *hmmlearn* (fork de *scikit-learn*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import MultinomialHMM\n",
    "\n",
    "# convert to int and reshape to 1D data\n",
    "X_train_id = [hmm1._convert_observations_sequence_to_index(sample) for sample in X_train]\n",
    "X_train_1D = np.atleast_2d(np.concatenate(X_train_id)).T\n",
    "X_train_lengths = [len(sample) for sample in X_train_id]\n",
    "\n",
    "X_test_id = [hmm1._convert_observations_sequence_to_index(sample) for sample in X_test]\n",
    "y_test_id = [hmm1._convert_observations_sequence_to_index(sample) for sample in y_test]\n",
    "X_test_1D = np.atleast_2d(np.concatenate(X_test_id)).T\n",
    "X_test_lengths = [len(sample) for sample in X_test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -467353.7960             +nan\n",
      "         2     -425509.8614      +41843.9346\n",
      "         3     -425237.2447        +272.6167\n",
      "         4     -424870.0098        +367.2349\n",
      "         5     -424341.9446        +528.0652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmmlearn score on test set\n",
      " * accuracy on full words : 0.00%\n",
      " * accuracy on letters    : 2.53%\n",
      "   > typos corrected      : 38 (0.52%)\n",
      "   > typos not corrected  : 707 (9.66%)\n",
      "   > typos added          : 6428 (87.81%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 62.89%\n",
      " * accuracy on letters    : 89.82%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 745 (10.18%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "hmm_sk = MultinomialHMM(26, verbose=True, n_iter=5, init_params='ste')\n",
    "hmm_sk.fit(X_train_1D, X_train_lengths)\n",
    "y_test_pred_sk = [hmm_sk.predict(np.atleast_2d(sample).T) for sample in X_test_id]\n",
    "y_test_pred = [hmm1._convert_states_sequence_to_string(sample) for sample in y_test_pred_sk]\n",
    "display_correction_stats(X_test, y_test, y_test_pred, name=\"hmmlearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: en fait, on ne peut pas comparer directement les sorties entre elles, puisque pas forcément de correspondance entre les états et les observations : un 'a' observé pourrait selon le HMM correspondre à l'état 'x'.  C'est donc normal que les résultats soient aussi mauvais.\n",
    "\n",
    "## Avec initialisation approximative de la matrice d'émission au préalable\n",
    "\n",
    "Afin de \"guider\" un peu l'apprentissage et surtout d'assurer la correspondance observations/états, nous avons testé d'initialiser la matrice d'émission à une valeur plus proche de la réalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st order HMM created with: \n",
      " * 26 states\n",
      " * 26 observations\n",
      "Unsupervised training by EM algorithm...\n",
      "EM LOOP: n_iter=1, delta=0.0015\n",
      "EM LOOP: n_iter=2, delta=0.0031\n",
      "EM LOOP: n_iter=3, delta=0.0019\n",
      "EM LOOP: n_iter=4, delta=0.0023\n",
      "EM LOOP: n_iter=5, delta=0.0013\n",
      "EM LOOP: n_iter=6, delta=0.0016\n",
      "EM LOOP: n_iter=7, delta=0.0015\n",
      "EM LOOP: n_iter=8, delta=0.0005\n",
      "EM LOOP: n_iter=9, delta=0.0005\n",
      "EM LOOP: n_iter=10, delta=0.0004\n",
      "\n",
      "HMM1 score on test set\n",
      " * accuracy on full words : 62.43%\n",
      " * accuracy on letters    : 89.39%\n",
      "   > typos corrected      : 4 (0.05%)\n",
      "   > typos not corrected  : 741 (10.12%)\n",
      "   > typos added          : 36 (0.49%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -466455.1652             +nan\n",
      "         2     -415163.1150      +51292.0501\n",
      "         3     -402473.4672      +12689.6478\n",
      "         4     -390475.5913      +11997.8759\n",
      "         5     -382688.7792       +7786.8121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmmlearn score on test set\n",
      " * accuracy on full words : 60.49%\n",
      " * accuracy on letters    : 88.54%\n",
      "   > typos corrected      : 75 (1.02%)\n",
      "   > typos not corrected  : 670 (9.15%)\n",
      "   > typos added          : 169 (2.31%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 62.89%\n",
      " * accuracy on letters    : 89.82%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 745 (10.18%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# emission prior\n",
    "emission0 = np.eye(26) + 1./26.\n",
    "emission0 /= np.sum(emission0, axis=1)\n",
    "\n",
    "# our code\n",
    "hmm1 = HMM(states, observations, verbose=True)\n",
    "hmm1.observation_logproba = np.log(emission0)\n",
    "hmm1.fit(X_train, max_iter=10, tol=0.0001)\n",
    "y_test_pred = hmm1.predict(X_test)\n",
    "display_correction_stats(X_test, y_test, y_test_pred, name=\"\\nHMM1\", dummy=False)\n",
    "\n",
    "# hmmlearn\n",
    "hmm_sk = MultinomialHMM(26, verbose=True, n_iter=5, init_params='st')\n",
    "hmm_sk.emissionprob_ = emission0\n",
    "hmm_sk.fit(X_train_1D, X_train_lengths)\n",
    "y_test_pred_sk = [hmm_sk.predict(np.atleast_2d(sample).T) for sample in X_test_id]\n",
    "y_test_pred = [hmm1._convert_states_sequence_to_string(sample) for sample in y_test_pred_sk]\n",
    "display_correction_stats(X_test, y_test, y_test_pred, name=\"hmmlearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Mieux, mais similaire au *Dummy*, donc inutile. Si on regarde les matrices estimées, on peut voir que la matrice d'émission initiale a été raffinnée, et que la matrice de transition (initialisée aléatoirement) semble bien retranscire certaines règles de la langue (un 't' est souvent suivi d'un 'h' etc.)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TC4-tp2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
