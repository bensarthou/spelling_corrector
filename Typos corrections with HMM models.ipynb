{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwILkW1F8FnJ"
   },
   "source": [
    "# Correction des typos\n",
    "\n",
    "Le but de ce projet est de corriger les fautes de frappes dans un texte sans recours à un dictionnaire, en utilisant un modèle de Markov caché (HMM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuO2bOCE8FnR"
   },
   "source": [
    "# I - Chargement des données\n",
    "\n",
    "Données issues du *Manifeste de l'Unabomber* et artificiellement bruitées en mofifiant aléatoirement 10% ou 20% des lettres du corpus (erreurs de substitutions seulement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from HMM import *\n",
    "from toolbox import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 states :\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "26 observations :\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "Sample example (observation, état) :\n",
      "[('a', 'a'), ('c', 'c'), ('v', 'c'), ('o', 'o'), ('u', 'u'), ('n', 'n'), ('t', 't')]\n"
     ]
    }
   ],
   "source": [
    "# Import and separate datasets\n",
    "ERROR_RATE = 10  # 10% or 20%\n",
    "train_set, test_set = load_db(error_rate=ERROR_RATE)\n",
    "X_train = [[token[0] for token in word] for word in train_set]\n",
    "y_train = [[token[1] for token in word] for word in train_set]\n",
    "X_test = [[token[0] for token in word] for word in test_set]\n",
    "y_test = [[token[1] for token in word] for word in test_set]\n",
    "\n",
    "# Get states and observations sets\n",
    "states, observations = get_observations_states(X_train, y_train)\n",
    "print(\"{} states :\\n{}\".format(len(states), states))\n",
    "print(\"{} observations :\\n{}\".format(len(observations), observations))\n",
    "\n",
    "# Example from dataset\n",
    "print(\"\\nSample example (observation, état) :\\n{}\".format(train_set[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - HMM d'ordre 1\n",
    "\n",
    "Essai de correction des typos par un HMM d'ordre 1 utilisant l'algorithme de Viterbi.\n",
    "On prend en compte un seul instant précédent pour le calcul de la transition. On considère donc les probabilités :\n",
    "- $P(X_t| Y_t)$ : probabilité d'observation de X à l'état Y à l'instant t\n",
    "- $P(Y_t| Y_{t-1})$ : probabilité de l'état Y à l'instant t sachant  l'état précédent\n",
    "\n",
    "Afin d'éviter les probabilités nulles, on initialise toutes les matrices (émission, transition et état initial) par une probabilité uniforme (respectivement $1/n_{observations}$, $1/n_{states}$, $1/n_{states}$), puis on affine ces probabilités grâce aux comptes selon le maximum de vraisemblance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st order HMM created with: \n",
      " * 26 states\n",
      " * 26 observations\n",
      "Training initial states probabilities... Done.\n",
      "Training transitions probabilities given states... Done.\n",
      "Training observations probabilities given states... Done.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train HMM\n",
    "hmm1 = HMM(states, observations)\n",
    "hmm1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation sequence      : inferikrigy\n",
      "Real states sequence      : inferiority\n",
      "Predicted states sequence : inderiorigy\n"
     ]
    }
   ],
   "source": [
    "# Try HMM prediction on one sample\n",
    "SAMPLE = 11\n",
    "observation_sequence = X_test[SAMPLE]\n",
    "states_sequence = y_test[SAMPLE]\n",
    "\n",
    "predicted_states_sequence = hmm1.predict([observation_sequence])\n",
    "\n",
    "print(\"Observation sequence      : {}\".format(\"\".join(observation_sequence)))\n",
    "print(\"Real states sequence      : {}\".format(\"\".join(states_sequence)))\n",
    "print(\"Predicted states sequence : {}\".format(\"\".join(predicted_states_sequence[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM1 score on test set\n",
      " * accuracy on full words : 75.15%\n",
      " * accuracy on letters    : 93.20%\n",
      "   > typos corrected      : 310 (4.23%)\n",
      "   > typos not corrected  : 435 (5.94%)\n",
      "   > typos added          : 63 (0.86%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 62.89%\n",
      " * accuracy on letters    : 89.82%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 745 (10.18%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = hmm1.predict(X_test)\n",
    "display_correction_stats(X_test, y_test, y_test_pred, name=\"HMM1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YO0E0GvD8FnS"
   },
   "source": [
    "\n",
    "# III -HMM d'ordre 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM d'ordre 2\n",
    "\n",
    "On prend en compte les deux instants précédents pour le calcul de la transition. On considère donc maitenant les probabilités :\n",
    "- $P(X_t| Y_t)$ : probabilité d'observation de X à l'état Y à l'instant t (identique à l'ordre 1)\n",
    "- $P(Y_t| Y_{t-1}, Y_{t-2})$ : probabilité de l'état Y à l'instant t sachant les deux états précédents.\n",
    "\n",
    "La dimension de la matrice de transition étant plus importante, certains trigrammes peuvent ne pas avoir été observées dans le corpus d'apprentissage. Afin d'éviter les probabilités nulles, on initialise donc toutes les matrices (émission, transition et état initial) par une probabilité uniforme (respectivement $1/n_{observations}$, $1/n_{states}$, $1/n_{states}$), puis on affine ces probabilités grâce aux comptes selon le maximum de vraisemblance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd order HMM created with: \n",
      " * 26 states\n",
      " * 26 observations\n",
      "Training initial states probabilities... Done.\n",
      "Training transitions probabilities given states... Done.\n",
      "Training observations probabilities given states... Done.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train HMM\n",
    "hmm2 = HMM2(states, observations)\n",
    "hmm2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation sequence      : inferikrigy\n",
      "Real states sequence      : inferiority\n",
      "Predicted states sequence : inferiority\n"
     ]
    }
   ],
   "source": [
    "# Try HMM prediction on one sample\n",
    "SAMPLE = 11\n",
    "observation_sequence = X_test[SAMPLE]\n",
    "states_sequence = y_test[SAMPLE]\n",
    "\n",
    "predicted_states_sequence = hmm2.predict([observation_sequence])\n",
    "\n",
    "print(\"Observation sequence      : {}\".format(\"\".join(observation_sequence)))\n",
    "print(\"Real states sequence      : {}\".format(\"\".join(states_sequence)))\n",
    "print(\"Predicted states sequence : {}\".format(\"\".join(predicted_states_sequence[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM2 score on test set\n",
      " * accuracy on full words : 83.41%\n",
      " * accuracy on letters    : 95.57%\n",
      "   > typos corrected      : 513 (7.01%)\n",
      "   > typos not corrected  : 232 (3.17%)\n",
      "   > typos added          : 92 (1.26%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 62.89%\n",
      " * accuracy on letters    : 89.82%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 745 (10.18%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = hmm2.predict(X_test)\n",
    "display_correction_stats(X_test, y_test, y_test_pred, name=\"HMM2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM d'ordre 2 avec smoothing\n",
    "\n",
    "Comme de nombreux trigrammes peuvent ne pas avoir été rencontrés dans le corpus d'apprentissage, on peut introduire différentes techniques de lissage ou d'extrapolation des données. On teste ici la technique introduite par [cet article](http://www.aclweb.org/anthology/P99-1023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd order HMM created with: \n",
      " * 26 states\n",
      " * 26 observations\n",
      "Training initial states probabilities... Done.\n",
      "Training transitions probabilities given states... Done.\n",
      "Training observations probabilities given states... Done.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train HMM\n",
    "hmm2s = HMM2(states, observations)\n",
    "hmm2s.fit(X_train, y_train, smoothing='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation sequence      : inferikrigy\n",
      "Real states sequence      : inferiority\n",
      "Predicted states sequence : inferiorigy\n"
     ]
    }
   ],
   "source": [
    "# Try HMM prediction on one sample\n",
    "SAMPLE = 11\n",
    "observation_sequence = X_test[SAMPLE]\n",
    "states_sequence = y_test[SAMPLE]\n",
    "\n",
    "predicted_states_sequence = hmm2s.predict([observation_sequence])\n",
    "\n",
    "print(\"Observation sequence      : {}\".format(\"\".join(observation_sequence)))\n",
    "print(\"Real states sequence      : {}\".format(\"\".join(states_sequence)))\n",
    "print(\"Predicted states sequence : {}\".format(\"\".join(predicted_states_sequence[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM2_smooth score on test set\n",
      " * accuracy on full words : 79.61%\n",
      " * accuracy on letters    : 94.66%\n",
      "   > typos corrected      : 406 (5.55%)\n",
      "   > typos not corrected  : 339 (4.63%)\n",
      "   > typos added          : 52 (0.71%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 62.89%\n",
      " * accuracy on letters    : 89.82%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 745 (10.18%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = hmm2s.predict(X_test)\n",
    "display_correction_stats(X_test, y_test, y_test_pred, name=\"HMM2_smooth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - Insertion de caractères\n",
    "\n",
    "Pour simuler l'insertion de caractères, on ajoute avec un certaine probabilité un caractère (dont la touche du clavier est proche de la précédente). Comme cette observation supplémentaire ne correspond pas un état (pas de véritable lettre associée à cette insertion), on modélise celui-ci par un nouvel état '\\_'. On se retrouve face à un problème de 27 états (26 lettres + '\\_') et 26 observations.\n",
    "\n",
    "Exemple:\n",
    "- **séquence d'états        :** ['a', 'c', 'c', 'o', '\\_', 'u', 'n', 't']\n",
    "- **séquence d'observations :** ['a', 'c', 'v', 'o', 'p', 'u', 'n', 't']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States sequence sample       : ['a', 'c', 'c', 'o', '_', 'u', 'n', 't']\n",
      "Observations sequence sample : ['a', 'c', 'v', 'o', 'o', 'u', 'n', 't']\n",
      "\n",
      "27 states :\n",
      "['_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "26 observations :\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Insert noisy observations (linked to no state) in the database\n",
    "# Empty state will be denoted '_'\n",
    "INSERTION_PROB = 0.05\n",
    "(X_ni_train, y_ni_train), (X_ni_test, y_ni_test) = noisy_insertion(X_train, y_train, X_test, y_test, thresh_proba=INSERTION_PROB)\n",
    "\n",
    "SAMPLE = 3\n",
    "print(\"States sequence sample       : {}\".format(y_ni_train[SAMPLE]))\n",
    "print(\"Observations sequence sample : {}\\n\".format(X_ni_train[SAMPLE]))\n",
    "\n",
    "states_ni, observations_ni = get_observations_states(X_ni_train, y_ni_train)\n",
    "print(\"{} states :\\n{}\".format(len(states_ni), states_ni))\n",
    "print(\"{} observations :\\n{}\".format(len(observations_ni), observations_ni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st order HMM created with: \n",
      " * 27 states\n",
      " * 26 observations\n",
      "Training initial states probabilities... Done.\n",
      "Training transitions probabilities given states... Done.\n",
      "Training observations probabilities given states... Done.\n",
      "\n",
      "HMM1 score on test set\n",
      " * accuracy on full words : 61.69%\n",
      " * accuracy on letters    : 88.04%\n",
      "   > typos corrected      : 328 (4.27%)\n",
      "   > typos not corrected  : 784 (10.20%)\n",
      "   > typos added          : 135 (1.76%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 51.63%\n",
      " * accuracy on letters    : 85.53%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 1112 (14.47%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# HMM first order\n",
    "hmm = HMM(states_ni, observations_ni)\n",
    "hmm.fit(X_ni_train, y_ni_train)\n",
    "y_ni_test_pred = hmm.predict(X_ni_test)\n",
    "display_correction_stats(X_ni_test, y_ni_test, y_ni_test_pred, name=\"\\nHMM1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd order HMM created with: \n",
      " * 27 states\n",
      " * 26 observations\n",
      "Training initial states probabilities... Done.\n",
      "Training transitions probabilities given states... Done.\n",
      "Training observations probabilities given states... Done.\n",
      "\n",
      "HMM2 score on test set\n",
      " * accuracy on full words : 69.55%\n",
      " * accuracy on letters    : 89.70%\n",
      "   > typos corrected      : 538 (7.00%)\n",
      "   > typos not corrected  : 574 (7.47%)\n",
      "   > typos added          : 218 (2.84%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 51.63%\n",
      " * accuracy on letters    : 85.53%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 1112 (14.47%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# HMM second order\n",
    "hmm2 = HMM2(states_ni, observations_ni)\n",
    "hmm2.fit(X_ni_train, y_ni_train)\n",
    "y_ni_test_pred = hmm2.predict(X_ni_test)\n",
    "display_correction_stats(X_ni_test, y_ni_test, y_ni_test_pred, name=\"\\nHMM2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd order HMM created with: \n",
      " * 27 states\n",
      " * 26 observations\n",
      "Training initial states probabilities... Done.\n",
      "Training transitions probabilities given states... Done.\n",
      "Training observations probabilities given states... Done.\n",
      "\n",
      "HMM2_smooth score on test set\n",
      " * accuracy on full words : 64.09%\n",
      " * accuracy on letters    : 89.40%\n",
      "   > typos corrected      : 390 (5.07%)\n",
      "   > typos not corrected  : 722 (9.39%)\n",
      "   > typos added          : 93 (1.21%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 51.63%\n",
      " * accuracy on letters    : 85.53%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 1112 (14.47%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# HMM second order with smoothing\n",
    "hmm2 = HMM2(states_ni, observations_ni)\n",
    "hmm2.fit(X_ni_train, y_ni_train, smoothing=True)\n",
    "y_ni_test_pred = hmm2.predict(X_ni_test)\n",
    "display_correction_stats(X_ni_test, y_ni_test, y_ni_test_pred, name=\"\\nHMM2_smooth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion :**\n",
    "\n",
    "Résultats assez décevants... En réalité, si on observe le comportement du modèle en présence de seules fautes d'insertions (et non insertions + substitutions), on remarque que les performance de celui-ci sont à peine meilleures que le *Dummy*. Ceci est probablement dû aux peu de données d'apprentissage disponibles pour ce type d'erreurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V - Omission de caractères\n",
    "\n",
    "Pour l'omission (délétion) de caractères, on est dans le cas contraire, où une observation est manquante. On ne peut simplement rajouter une observation '\\_', puisque cela reviendrait à indiquer directement au modèle qu'il y a eu une omission.  On serait alors dans un cas plus simple, mais où le modèle n'aurait pas à déterminer tout seul la résence d'une omission de caractère. Pour ceci, on modifie plutôt le corpus en supprimant certaines observations, et en autorisant le remplacement d'un observation par un état \"double\", composé de deux lettres. On se retrouve face à un problème beaucoup plus complexe, de 702 états (26 états simples + 676 états doubles) et 26 observations.\n",
    "\n",
    "Exemple:\n",
    "- **séquence d'états        :** ['a', 'c', 'c', 'o', 'un', 't']\n",
    "- **séquence d'observations :** ['a', 'c', 'v', 'o', 'u', 't']\n",
    "\n",
    "Remarque: de nombreuses lignes de la matrice de transition seront vides, puisque chacune de ces fautes est rarement observée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_states(states):\n",
    "    \"\"\" Take a state list as input and return a cross state list\n",
    "    (each new state being a concatenation of two states from states, or a unique state)\n",
    "    Ex: If states=['a', 'b'], cross_states is ['a', 'b', 'aa', 'ab', 'ba', 'bb']\n",
    "\n",
    "    :param states, list of string\n",
    "    :return cross_states: states, list of string\n",
    "    \"\"\"\n",
    "    \n",
    "    cross_states = []\n",
    "    \n",
    "    for s1 in states:\n",
    "        for s2 in states:\n",
    "            cross_states.append(s1 + s2)\n",
    "            \n",
    "    return states + cross_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702 states :\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az', 'ba', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bh', 'bi', 'bj', 'bk', 'bl', 'bm', 'bn', 'bo', 'bp', 'bq', 'br', 'bs', 'bt', 'bu', 'bv', 'bw', 'bx', 'by', 'bz', 'ca', 'cb', 'cc', 'cd', 'ce', 'cf', 'cg', 'ch', 'ci', 'cj', 'ck', 'cl', 'cm', 'cn', 'co', 'cp', 'cq', 'cr', 'cs', 'ct', 'cu', 'cv', 'cw', 'cx', 'cy', 'cz', 'da', 'db', 'dc', 'dd', 'de', 'df', 'dg', 'dh', 'di', 'dj', 'dk', 'dl', 'dm', 'dn', 'do', 'dp', 'dq', 'dr', 'ds', 'dt', 'du', 'dv', 'dw', 'dx', 'dy', 'dz', 'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'eh', 'ei', 'ej', 'ek', 'el', 'em', 'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ew', 'ex', 'ey', 'ez', 'fa', 'fb', 'fc', 'fd', 'fe', 'ff', 'fg', 'fh', 'fi', 'fj', 'fk', 'fl', 'fm', 'fn', 'fo', 'fp', 'fq', 'fr', 'fs', 'ft', 'fu', 'fv', 'fw', 'fx', 'fy', 'fz', 'ga', 'gb', 'gc', 'gd', 'ge', 'gf', 'gg', 'gh', 'gi', 'gj', 'gk', 'gl', 'gm', 'gn', 'go', 'gp', 'gq', 'gr', 'gs', 'gt', 'gu', 'gv', 'gw', 'gx', 'gy', 'gz', 'ha', 'hb', 'hc', 'hd', 'he', 'hf', 'hg', 'hh', 'hi', 'hj', 'hk', 'hl', 'hm', 'hn', 'ho', 'hp', 'hq', 'hr', 'hs', 'ht', 'hu', 'hv', 'hw', 'hx', 'hy', 'hz', 'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'ih', 'ii', 'ij', 'ik', 'il', 'im', 'in', 'io', 'ip', 'iq', 'ir', 'is', 'it', 'iu', 'iv', 'iw', 'ix', 'iy', 'iz', 'ja', 'jb', 'jc', 'jd', 'je', 'jf', 'jg', 'jh', 'ji', 'jj', 'jk', 'jl', 'jm', 'jn', 'jo', 'jp', 'jq', 'jr', 'js', 'jt', 'ju', 'jv', 'jw', 'jx', 'jy', 'jz', 'ka', 'kb', 'kc', 'kd', 'ke', 'kf', 'kg', 'kh', 'ki', 'kj', 'kk', 'kl', 'km', 'kn', 'ko', 'kp', 'kq', 'kr', 'ks', 'kt', 'ku', 'kv', 'kw', 'kx', 'ky', 'kz', 'la', 'lb', 'lc', 'ld', 'le', 'lf', 'lg', 'lh', 'li', 'lj', 'lk', 'll', 'lm', 'ln', 'lo', 'lp', 'lq', 'lr', 'ls', 'lt', 'lu', 'lv', 'lw', 'lx', 'ly', 'lz', 'ma', 'mb', 'mc', 'md', 'me', 'mf', 'mg', 'mh', 'mi', 'mj', 'mk', 'ml', 'mm', 'mn', 'mo', 'mp', 'mq', 'mr', 'ms', 'mt', 'mu', 'mv', 'mw', 'mx', 'my', 'mz', 'na', 'nb', 'nc', 'nd', 'ne', 'nf', 'ng', 'nh', 'ni', 'nj', 'nk', 'nl', 'nm', 'nn', 'no', 'np', 'nq', 'nr', 'ns', 'nt', 'nu', 'nv', 'nw', 'nx', 'ny', 'nz', 'oa', 'ob', 'oc', 'od', 'oe', 'of', 'og', 'oh', 'oi', 'oj', 'ok', 'ol', 'om', 'on', 'oo', 'op', 'oq', 'or', 'os', 'ot', 'ou', 'ov', 'ow', 'ox', 'oy', 'oz', 'pa', 'pb', 'pc', 'pd', 'pe', 'pf', 'pg', 'ph', 'pi', 'pj', 'pk', 'pl', 'pm', 'pn', 'po', 'pp', 'pq', 'pr', 'ps', 'pt', 'pu', 'pv', 'pw', 'px', 'py', 'pz', 'qa', 'qb', 'qc', 'qd', 'qe', 'qf', 'qg', 'qh', 'qi', 'qj', 'qk', 'ql', 'qm', 'qn', 'qo', 'qp', 'qq', 'qr', 'qs', 'qt', 'qu', 'qv', 'qw', 'qx', 'qy', 'qz', 'ra', 'rb', 'rc', 'rd', 're', 'rf', 'rg', 'rh', 'ri', 'rj', 'rk', 'rl', 'rm', 'rn', 'ro', 'rp', 'rq', 'rr', 'rs', 'rt', 'ru', 'rv', 'rw', 'rx', 'ry', 'rz', 'sa', 'sb', 'sc', 'sd', 'se', 'sf', 'sg', 'sh', 'si', 'sj', 'sk', 'sl', 'sm', 'sn', 'so', 'sp', 'sq', 'sr', 'ss', 'st', 'su', 'sv', 'sw', 'sx', 'sy', 'sz', 'ta', 'tb', 'tc', 'td', 'te', 'tf', 'tg', 'th', 'ti', 'tj', 'tk', 'tl', 'tm', 'tn', 'to', 'tp', 'tq', 'tr', 'ts', 'tt', 'tu', 'tv', 'tw', 'tx', 'ty', 'tz', 'ua', 'ub', 'uc', 'ud', 'ue', 'uf', 'ug', 'uh', 'ui', 'uj', 'uk', 'ul', 'um', 'un', 'uo', 'up', 'uq', 'ur', 'us', 'ut', 'uu', 'uv', 'uw', 'ux', 'uy', 'uz', 'va', 'vb', 'vc', 'vd', 've', 'vf', 'vg', 'vh', 'vi', 'vj', 'vk', 'vl', 'vm', 'vn', 'vo', 'vp', 'vq', 'vr', 'vs', 'vt', 'vu', 'vv', 'vw', 'vx', 'vy', 'vz', 'wa', 'wb', 'wc', 'wd', 'we', 'wf', 'wg', 'wh', 'wi', 'wj', 'wk', 'wl', 'wm', 'wn', 'wo', 'wp', 'wq', 'wr', 'ws', 'wt', 'wu', 'wv', 'ww', 'wx', 'wy', 'wz', 'xa', 'xb', 'xc', 'xd', 'xe', 'xf', 'xg', 'xh', 'xi', 'xj', 'xk', 'xl', 'xm', 'xn', 'xo', 'xp', 'xq', 'xr', 'xs', 'xt', 'xu', 'xv', 'xw', 'xx', 'xy', 'xz', 'ya', 'yb', 'yc', 'yd', 'ye', 'yf', 'yg', 'yh', 'yi', 'yj', 'yk', 'yl', 'ym', 'yn', 'yo', 'yp', 'yq', 'yr', 'ys', 'yt', 'yu', 'yv', 'yw', 'yx', 'yy', 'yz', 'za', 'zb', 'zc', 'zd', 'ze', 'zf', 'zg', 'zh', 'zi', 'zj', 'zk', 'zl', 'zm', 'zn', 'zo', 'zp', 'zq', 'zr', 'zs', 'zt', 'zu', 'zv', 'zw', 'zx', 'zy', 'zz']\n",
      "26 observations :\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Remove some observations in the database\n",
    "# To do that, we add the state without observation to the previous state\n",
    "DELETION_PROB = 0.05\n",
    "(X_no_train, y_no_train), (X_no_test, y_no_test) = noisy_omission(X_train, y_train, X_test, y_test, thresh_proba=DELETION_PROB)\n",
    "\n",
    "states_no = cross_states(states)\n",
    "observations_no = observations\n",
    "\n",
    "print(\"{} states :\\n{}\".format(len(states_no), states_no))\n",
    "print(\"{} observations :\\n{}\".format(len(observations_no), observations_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'v', 'o', 'u', 'n', 't']\n",
      "['ac', 'c', 'o', 'u', 'n', 't']\n"
     ]
    }
   ],
   "source": [
    "sample = 3\n",
    "print(X_no_train[sample])\n",
    "print(y_no_train[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st order HMM created with: \n",
      " * 702 states\n",
      " * 26 observations\n",
      "Training initial states probabilities... Done.\n",
      "Training transitions probabilities given states... Done.\n",
      "Training observations probabilities given states... Done.\n",
      "HMM score on test set\n",
      " * accuracy on full words : 62.89%\n",
      " * accuracy on letters    : 88.96%\n",
      "   > typos corrected      : 290 (4.17%)\n",
      "   > typos not corrected  : 699 (10.06%)\n",
      "   > typos added          : 68 (0.98%)\n",
      "\n",
      "Dummy score on test set\n",
      " * accuracy on full words : 52.83%\n",
      " * accuracy on letters    : 85.77%\n",
      "   > typos corrected      : 0 (0.00%)\n",
      "   > typos not corrected  : 989 (14.23%)\n",
      "   > typos added          : 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# HMM first order\n",
    "hmm = HMM(states_no, observations_no)\n",
    "hmm.fit(X_no_train, y_no_train)\n",
    "y_no_test_pred = hmm.predict(X_no_test)\n",
    "display_correction_stats(X_no_test, y_no_test, y_no_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**/!\\ Attention ! /!\\**\n",
    "\n",
    "Attention, dans le cas de l'ordre 2, les matrices considérées peuvent prendre des tailles très importantes. Ici, la matrice de transition est de taille 702 x 702 x 702, ce qui dans le cas de flottants sur 64 bits (type de base d'un array numpy), revient à une matrice de plus de 2.5 Go. Afin d'éviter des erreurs de mémoire et limiter les calculs, la précision des nombres flottants est automatiquement modifiée en fonction de la dimension du problème, aboutissant ici à des matrices de \"seulement\" 600 Mo. Pour compenser cette perte de pécision, l'usage des log-probabilités est fortement recommandé (et effectué ici), permettant de mieux exploiter la plage des valeurs possibles, et ainsi limiter les erreurs de représentations.\n",
    "\n",
    "Malgré l'utilisation du *broadcasting* numpy, l'algorithme de Viterbi reste très lent, avec plus d'une dizaine de secondes par séquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd order HMM created with: \n",
      " * 702 states\n",
      " * 26 observations\n",
      "Training initial states probabilities... Done.\n",
      "Training transitions probabilities given states... Done.\n",
      "Training observations probabilities given states... Done.\n"
     ]
    }
   ],
   "source": [
    "# HMM second order\n",
    "hmm2 = HMM2(states_no, observations_no)\n",
    "hmm2.fit(X_no_train, y_no_train)\n",
    "y_no_test_pred = hmm2.predict(X_no_test)\n",
    "display_correction_stats(X_no_test, y_no_test, y_no_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM second order with smoothing\n",
    "hmm2s = HMM2(states_no, observations_no)\n",
    "hmm2s.fit(X_no_train, y_no_train, smoothing=True)\n",
    "y_no_test_pred = hmm2s.predict(X_no_test)\n",
    "display_correction_stats(X_no_test, y_no_test, y_no_test_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TC4-tp2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
